## step4 fentry 

```c
struct {__uint(type, BPF_MAP_TYPE_RINGBUF);} events SEC(".maps");

SEC("fentry/tcp_connect")
int BPF_PROG(tcp_connect, struct sock *sk) {
    bpf_ringbuf_reserve(&events, sizeof(struct event), 0);
    bpf_get_current_comm(&tcp_info->comm, TASK_COMM_LEN);
	bpf_ringbuf_submit(tcp_info, 0);
}
```

```go
func main() {	
	if err := rlimit.RemoveMemlock(); err != nil {}
	if err := loadFentryObjects(&objs, nil); err != nil {}
	link, err := link.AttachTracing(link.TracingOptions{})
	rd, err := ringbuf.NewReader(objs.fentryMaps.Events)	
	for {
		record, err := rd.Read()
		binary.Read(bytes.NewBuffer(record.RawSample), binary.BigEndian, &event); err != nil {
	}
}
```

##  step5 array map kprobe 

```c
struct {__uint(type, BPF_MAP_TYPE_ARRAY); } kprobe_map SEC(".maps"); 

SEC("kprobe/sys_execve")
int kprobe_execve() {
	valp = bpf_map_lookup_elem(&kprobe_map, &key);	
	bpf_map_update_elem(&kprobe_map, &key, &initval, BPF_ANY);		
	__sync_fetch_and_add(valp, 1);
	return 0;
}
```

```go
func main() {
    fn := "sys_execve"
	err := rlimit.RemoveMemlock();
	err := loadBpfObjects(&objs, nil); 
	kp, err := link.Kprobe(fn, objs.KprobeExecve, nil){}	
	for range ticker.C {		
		if err := objs.KprobeMap.Lookup(mapKey, &value); err != nil {}		
	}
}
```

## step6 kprobe.map

```c
struct { __uint(type, BPF_MAP_TYPE_PERCPU_ARRAY); } kprobe_map SEC(".maps"); 

SEC("kprobe/sys_execve")
int kprobe_execve() {
	valp = bpf_map_lookup_elem(&kprobe_map, &key);
	bpf_map_update_elem(&kprobe_map, &key, &initval, BPF_ANY);
	__sync_fetch_and_add(valp, 1);
	return 0;
}
```

```go
func main() {	
	fn := "sys_execve"
	err := loadBpfObjects(&objs, nil); 
	kp, err := link.Kprobe(fn, objs.KprobeExecve, nil)
	for range ticker.C {
		if err := objs.KprobeMap.Lookup(mapKey, &all_cpu_value); err != nil {}		
	}
}
```

## step7 ringbuffer
```c
struct {__uint(type, BPF_MAP_TYPE_RINGBUF);} events SEC(".maps");

SEC("kprobe/sys_execve")
int kprobe_execve(struct pt_regs *ctx) {
	u64 id   = bpf_get_current_pid_tgid();
	task_info = bpf_ringbuf_reserve(&events, sizeof(struct event), 0);
	bpf_get_current_comm(&task_info->comm, 80);
	bpf_ringbuf_submit(task_info, 0);
	return 0;
}
```

```go
func main() {
	fn := "sys_execve"
	err := loadBpfObjects(&objs, nil); 
	kp, err := link.Kprobe(fn, objs.KprobeExecve, nil)	
	rd, err := ringbuf.NewReader(objs.Events)
    var event bpfEvent
	for {
		record, err := rd.Read()		
		err := binary.Read(bytes.NewBuffer(record.RawSample), binary.LittleEndian, &event); 
		log.Printf("pid: %d\tcomm: %s\n", event.Pid, unix.ByteSliceToString(event.Comm[:]))
	}
}
```            

## step8 tcprtt
```c

struct {__uint(type, BPF_MAP_TYPE_RINGBUF);} events SEC(".maps");
struct event {};
struct event *unused_event __attribute__((unused));

SEC("fentry/tcp_close")
int BPF_PROG(tcp_close, struct sock *sk) {
	if (sk->__sk_common.skc_family != AF_INET) {}

	struct tcp_sock *ts = bpf_skc_to_tcp_sock(sk);
	struct event *tcp_info;
	tcp_info = bpf_ringbuf_reserve(&events, sizeof(struct event), 0);

	tcp_info->saddr = sk->__sk_common.skc_rcv_saddr;
	tcp_info->daddr = sk->__sk_common.skc_daddr;
	tcp_info->dport = bpf_ntohs(sk->__sk_common.skc_dport);
	tcp_info->sport = sk->__sk_common.skc_num;
	tcp_info->srtt = ts->srtt_us >> 3;
	tcp_info->srtt /= 1000;
	bpf_ringbuf_submit(tcp_info, 0);

	return 0;
}
```

```go
func main() {
	err := loadBpfObjects(&objs, nil); 
	link, err := link.AttachTracing(link.TracingOptions{ Program: objs.bpfPrograms.TcpClose,})	
	rd, err := ringbuf.NewReader(objs.bpfMaps.Events)	
	go readLoop(rd)	
}

func readLoop(rd *ringbuf.Reader) {	
	var event bpfEvent
	for {
		record, err := rd.Read()			
		binary.Read(bytes.NewBuffer(record.RawSample), binary.LittleEndian, &event); 
		log.Printf("%-15s %-6d -> %-15s %-6d %-6d",...		)
	}
}
```

## step9 tcprtt_sockops
```c
struct {} rtt_events SEC(".maps");
struct rtt_event {};
struct rtt_event *unused_event __attribute__((unused));

static inline void init_sk_key(struct bpf_sock_ops *skops, struct sk_key *sk_key) {..}
static inline void bpf_sock_ops_establish_cb(struct bpf_sock_ops *skops, u8 sock_type) {
	err = bpf_map_update_elem(&map_estab_sk, &sk_info.sk_key, &sk_info, BPF_NOEXIST);
	bpf_sock_ops_cb_flags_set(skops, BPF_SOCK_OPS_RTT_CB_FLAG | BPF_SOCK_OPS_STATE_CB_FLAG);
}

static inline void bpf_sock_ops_rtt_cb(struct bpf_sock_ops *skops) {
	sk_info = bpf_map_lookup_elem(&map_estab_sk, &sk_key);
	rtt_event = bpf_ringbuf_reserve(&rtt_events, sizeof(struct rtt_event), 0);

	switch (sk_info->sk_type) {
	case SOCK_TYPE_ACTIVE:	
	case SOCK_TYPE_PASSIVE:
	}
	bpf_ringbuf_submit(rtt_event, 0);
}

static inline void bpf_sock_ops_state_cb(struct bpf_sock_ops *skops) {}

SEC("sockops")
int bpf_sockops_cb(struct bpf_sock_ops *skops) {
	u32 op;
	op = skops->op;

	switch (op) {
	case BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB:
		bpf_sock_ops_establish_cb(skops, SOCK_TYPE_ACTIVE);
		break;
	case BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB:
		bpf_sock_ops_establish_cb(skops, SOCK_TYPE_PASSIVE);
		break;
	case BPF_SOCK_OPS_RTT_CB:
		bpf_sock_ops_rtt_cb(skops);
		break;
	case BPF_SOCK_OPS_STATE_CB:
		bpf_sock_ops_state_cb(skops);
		break;
	}
	return 0;
}
```

```go
func main() {
	
	err := rlimit.RemoveMemlock(); 
	err := loadBpfObjects(&objs, nil); 
	link, err := link.AttachCgroup(link.CgroupOptions{})	
	rd, err := ringbuf.NewReader(objs.bpfMaps.RttEvents)
	go readLoop(rd)	
	<-stopper
}

func readLoop(rd *ringbuf.Reader) {
	var event bpfRttEvent
	for {
		record, err := rd.Read()		
		err := binary.Read(bytes.NewBuffer(record.RawSample), binary.LittleEndian, &event); 
		log.Printf("%-15s %-6d -> %-15s %-6d %-6d",...)
	}
}
```


## step10 tracepoint
```c
struct {__uint(type, BPF_MAP_TYPE_ARRAY); } counting_map SEC(".maps"); 

// /sys/kernel/tracing/events/kmem/mm_page_alloc/format
struct alloc_info {
	unsigned long pad;
	unsigned long pfn;
	unsigned int order;
	unsigned int gfp_flags;
	int migratetype;
};

// This tracepoint is defined in mm/page_alloc.c:__alloc_pages_nodemask()
// Userspace pathname: /sys/kernel/tracing/events/kmem/mm_page_alloc
SEC("tracepoint/kmem/mm_page_alloc")
int mm_page_alloc(struct alloc_info *info) {
	u32 key     = 0;
	u64 initval = 1, *valp;

	valp = bpf_map_lookup_elem(&counting_map, &key);
	bpf_map_update_elem(&counting_map, &key, &initval, BPF_ANY);
	__sync_fetch_and_add(valp, 1);
	return 0;
}
```

```go
func main() {
	err := rlimit.RemoveMemlock(); 
	err := loadBpfObjects(&objs, nil);	
	kp, err := link.Tracepoint("kmem", "mm_page_alloc", objs.MmPageAlloc, nil)
	ticker := time.NewTicker(1 * time.Second)	
	for range ticker.C {		
		err := objs.CountingMap.Lookup(mapKey, &value); 
		log.Printf("%v times", value)
	}
}
```
## step 11 perf uretprobe
```c
struct {} events SEC(".maps");
struct event {};
const struct event *unused __attribute__((unused));

SEC("uretprobe/bash_readline")
int uretprobe_bash_readline(struct pt_regs *ctx) {
	struct event event;
	event.pid = bpf_get_current_pid_tgid();
	bpf_probe_read(&event.line, sizeof(event.line), (void *)PT_REGS_RC(ctx));
	bpf_perf_event_output(ctx, &events, BPF_F_CURRENT_CPU, &event, sizeof(event));
	return 0;
}
```

```go
func main() {	
	err := rlimit.RemoveMemlock();
	err := loadBpfObjects(&objs, nil);
	ex, err := link.OpenExecutable(binPath)
	up, err := ex.Uretprobe(symbol, objs.UretprobeBashReadline, nil)
	rd, err := perf.NewReader(objs.Events, os.Getpagesize())
	go func() {		
		<-stopper
    }()
	
	// bpfEvent is generated by bpf2go.
	var event bpfEvent
	for {
		record, err := rd.Read()
		err := binary.Read(bytes.NewBuffer(record.RawSample), binary.LittleEndian, &event); 
		log.Printf("%s:%s return value: %s", binPath, symbol, unix.ByteSliceToString(event.Line[:]))
	}
}
